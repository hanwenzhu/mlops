Here we describe CK automation to run MLPerf inference benchmarking using native MLPerf scripts
with CK packages for frameworks, data sets and models.

* [Image classification](task-image-classification.md)
* [Object detection](task-object-detection.md)
* [Medical imaging ](task-medical-imaging.md)
* [NLP](task-nlp.md)
* [Recommendation](task-recommendation.md)
* [Speech recognition](task-speech-recognition.md)




* [CK datasets](../datasets/README.md)





# Unsorted related to CK automation

* [Notes about models (issues, quantization, etc)](../models/notes.md)

* DLRM: [notes](dlrm.md), [CK packages](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+dlrm), [CK workflows](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+dlrm)
* [Search for CK program workflows with "mlperf"](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+mlperf)
* [Search for CK program workflows with "loadgen"](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+loadgen)

# Feedback
Ð¡ontact [Grigori Fursin](https://cKnowledge.io/@gfursin)
